{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MNIST classification by PaddlePaddle\n",
        "\n",
        "![screencast](https://cloud.githubusercontent.com/assets/80381/11339453/f04f885e-923c-11e5-8845-33c16978c54d.gif)\n",
        "\n",
        "## Usage\n",
        "\n",
        "This MNIST classification demo consists of two parts: a PaddlePaddle\n",
        "inference server and a Javascript front end. We will start them\n",
        "separately.\n",
        "\n",
        "We will use Docker to run the demo, if you are not familiar with\n",
        "Docker, please checkout\n",
        "this\n",
        "[tutorial](https://github.com/PaddlePaddle/Paddle/wiki/TLDR-for-new-docker-user).\n",
        "\n",
        "### Start the Inference Server\n",
        "\n",
        "The inference server can be used to inference any model trained by\n",
        "PaddlePaddle. Please see [here](../serve/README.md) for more details.\n",
        "\n",
        "1. Download the MNIST inference model topylogy and parameters to the\n",
        "   current working directory.\n",
        "\n",
        "    ```bash\n",
        "    wget https://s3.us-east-2.amazonaws.com/models.paddlepaddle/end-to-end-mnist/inference_topology.pkl\n",
        "    wget https://s3.us-east-2.amazonaws.com/models.paddlepaddle/end-to-end-mnist/param.tar\n",
        "    ```\n",
        "\n",
        "1. Run following command to start the inference server:\n",
        "\n",
        "    ```bash\n",
        "    docker run --name paddle_serve -v `pwd`:/data -d -p 8000:80 -e WITH_GPU=0 paddlepaddle/book:serve\n",
        "    ```\n",
        "\n",
        "    The above command will mount the current working directory to the\n",
        "    `/data` directory inside the docker container. The inference\n",
        "    server will load the model topology and parameters that we just\n",
        "    downloaded from there.\n",
        "\n",
        "    After you are done with the demo, you can run `docker stop\n",
        "    paddle_serve` to stop this docker container.\n",
        "\n",
        "### Start the Front End\n",
        "\n",
        "1. Run the following command\n",
        "   ```bash\n",
        "   docker run -it -p 5000:5000 -e BACKEND_URL=http://localhost:8000/ paddlepaddle/book:mnist\n",
        "   ```\n",
        "\n",
        "   `BACKEND_URL` in the above command specifies the inference server\n",
        "   endpoint. If you started the inference server on another machine,\n",
        "   or want to visit the front end remotely, you may want to change its\n",
        "   value.\n",
        "\n",
        "1. Visit http://localhost:5000 and you will see the PaddlePaddle MNIST demo.\n",
        "\n",
        "\n",
        "## Build\n",
        "\n",
        "We have already prepared the pre-built docker image\n",
        "`paddlepaddle/book:mnist`, here is the command if you want to build\n",
        "the docker image again.\n",
        "\n",
        "```bash\n",
        "docker build -t paddlepaddle/book:mnist .\n",
        "```\n",
        "\n",
        "\n",
        "## Acknowledgement\n",
        "\n",
        "Thanks to the great project https://github.com/sugyan/tensorflow-mnist\n",
        ". Most of the code in this project comes from there.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
