{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 情感分析\n",
        "\n",
        "本教程源代码目录在[book/understand_sentiment](https://github.com/PaddlePaddle/book/tree/develop/06.understand_sentiment)， 初次使用请参考PaddlePaddle[安装教程](https://github.com/PaddlePaddle/book/blob/develop/README.cn.md#运行这本书)，更多内容请参考本教程的[视频课堂](http://bit.baidu.com/course/detail/id/177.html)。\n",
        "\n",
        "## 背景介绍\n",
        "\n",
        "在自然语言处理中，情感分析一般是指判断一段文本所表达的情绪状态。其中，一段文本可以是一个句子，一个段落或一个文档。情绪状态可以是两类，如（正面，负面），（高兴，悲伤）；也可以是三类，如（积极，消极，中性）等等。情感分析的应用场景十分广泛，如把用户在购物网站（亚马逊、天猫、淘宝等）、旅游网站、电影评论网站上发表的评论分成正面评论和负面评论；或为了分析用户对于某一产品的整体使用感受，抓取产品的用户评论并进行情感分析等等。表格1展示了对电影评论进行情感分析的例子：\n",
        "\n",
        "| 电影评论       | 类别  |\n",
        "| --------     | -----  |\n",
        "| 在冯小刚这几年的电影里，算最好的一部的了| 正面 |\n",
        "| 很不好看，好像一个地方台的电视剧     | 负面 |\n",
        "| 圆方镜头全程炫技，色调背景美则美矣，但剧情拖沓，口音不伦不类，一直努力却始终无法入戏| 负面|\n",
        "|剧情四星。但是圆镜视角加上婺源的风景整个非常有中国写意山水画的感觉，看得实在太舒服了。。|正面|\n",
        "\n",
        "\u003cp align=\"center\"\u003e表格 1 电影评论情感分析\u003c/p\u003e\n",
        "\n",
        "在自然语言处理中，情感分析属于典型的**文本分类**问题，即把需要进行情感分析的文本划分为其所属类别。文本分类涉及文本表示和分类方法两个问题。在深度学习的方法出现之前，主流的文本表示方法为词袋模型BOW(bag of words)，话题模型等等；分类方法有SVM(support vector machine), LR(logistic regression)等等。  \n",
        "\n",
        "对于一段文本，BOW表示会忽略其词顺序、语法和句法，将这段文本仅仅看做是一个词集合，因此BOW方法并不能充分表示文本的语义信息。例如，句子“这部电影糟糕透了”和“一个乏味，空洞，没有内涵的作品”在情感分析中具有很高的语义相似度，但是它们的BOW表示的相似度为0。又如，句子“一个空洞，没有内涵的作品”和“一个不空洞而且有内涵的作品”的BOW相似度很高，但实际上它们的意思很不一样。  \n",
        "\n",
        "本章我们所要介绍的深度学习模型克服了BOW表示的上述缺陷，它在考虑词顺序的基础上把文本映射到低维度的语义空间，并且以端对端（end to end）的方式进行文本表示及分类，其性能相对于传统方法有显著的提升\\[[1](#参考文献)\\]。\n",
        "\n",
        "## 模型概览\n",
        "本章所使用的文本表示模型为卷积神经网络（Convolutional Neural Networks）和循环神经网络(Recurrent Neural Networks)及其扩展。下面依次介绍这几个模型。\n",
        "\n",
        "### 文本卷积神经网络简介（CNN）\n",
        "\n",
        "我们在[推荐系统](https://github.com/PaddlePaddle/book/tree/develop/05.recommender_system)一节介绍过应用于文本数据的卷积神经网络模型的计算过程，这里进行一个简单的回顾。\n",
        "\n",
        "对卷积神经网络来说，首先使用卷积处理输入的词向量序列，产生一个特征图（feature map），对特征图采用时间维度上的最大池化（max pooling over time）操作得到此卷积核对应的整句话的特征，最后，将所有卷积核得到的特征拼接起来即为文本的定长向量表示，对于文本分类问题，将其连接至softmax即构建出完整的模型。在实际应用中，我们会使用多个卷积核来处理句子，窗口大小相同的卷积核堆叠起来形成一个矩阵，这样可以更高效的完成运算。另外，我们也可使用窗口大小不同的卷积核来处理句子，[推荐系统](https://github.com/PaddlePaddle/book/tree/develop/05.recommender_system)一节的图3作为示意画了四个卷积核，不同颜色表示不同大小的卷积核操作。\n",
        "\n",
        "对于一般的短文本分类问题，上文所述的简单的文本卷积网络即可达到很高的正确率\\[[1](#参考文献)\\]。若想得到更抽象更高级的文本特征表示，可以构建深层文本卷积神经网络\\[[2](#参考文献),[3](#参考文献)\\]。\n",
        "\n",
        "### 循环神经网络（RNN）\n",
        "\n",
        "循环神经网络是一种能对序列数据进行精确建模的有力工具。实际上，循环神经网络的理论计算能力是图灵完备的\\[[4](#参考文献)\\]。自然语言是一种典型的序列数据（词序列），近年来，循环神经网络及其变体（如long short term memory\\[[5](#参考文献)\\]等）在自然语言处理的多个领域，如语言模型、句法解析、语义角色标注（或一般的序列标注）、语义表示、图文生成、对话、机器翻译等任务上均表现优异甚至成为目前效果最好的方法。\n",
        "\n",
        "\u003cp align=\"center\"\u003e\n",
        "\u003cimg src=\"image/rnn.png\" width = \"60%\" align=\"center\"/\u003e\u003cbr/\u003e\n",
        "图1. 循环神经网络按时间展开的示意图\n",
        "\u003c/p\u003e\n",
        "\n",
        "循环神经网络按时间展开后如图1所示：在第$t$时刻，网络读入第$t$个输入$x_t$（向量表示）及前一时刻隐层的状态值$h_{t-1}$（向量表示，$h_0$一般初始化为$0$向量），计算得出本时刻隐层的状态值$h_t$，重复这一步骤直至读完所有输入。如果将循环神经网络所表示的函数记为$f$，则其公式可表示为：\n",
        "\n",
        "$$h_t=f(x_t,h_{t-1})=\\sigma(W_{xh}x_t+W_{hh}h_{h-1}+b_h)$$\n",
        "\n",
        "其中$W_{xh}$是输入到隐层的矩阵参数，$W_{hh}$是隐层到隐层的矩阵参数，$b_h$为隐层的偏置向量（bias）参数，$\\sigma$为$sigmoid$函数。  \n",
        "\n",
        "在处理自然语言时，一般会先将词（one-hot表示）映射为其词向量（word embedding）表示，然后再作为循环神经网络每一时刻的输入$x_t$。此外，可以根据实际需要的不同在循环神经网络的隐层上连接其它层。如，可以把一个循环神经网络的隐层输出连接至下一个循环神经网络的输入构建深层（deep or stacked）循环神经网络，或者提取最后一个时刻的隐层状态作为句子表示进而使用分类模型等等。  \n",
        "\n",
        "### 长短期记忆网络（LSTM）\n",
        "\n",
        "对于较长的序列数据，循环神经网络的训练过程中容易出现梯度消失或爆炸现象\\[[6](#参考文献)\\]。为了解决这一问题，Hochreiter S, Schmidhuber J. (1997)提出了LSTM(long short term memory\\[[5](#参考文献)\\])。  \n",
        "\n",
        "相比于简单的循环神经网络，LSTM增加了记忆单元$c$、输入门$i$、遗忘门$f$及输出门$o$。这些门及记忆单元组合起来大大提升了循环神经网络处理长序列数据的能力。若将基于LSTM的循环神经网络表示的函数记为$F$，则其公式为：\n",
        "\n",
        "$$ h_t=F(x_t,h_{t-1})$$\n",
        "\n",
        "$F$由下列公式组合而成\\[[7](#参考文献)\\]：\n",
        "\\begin{align}\n",
        "i_t \u0026 = \\sigma(W_{xi}x_t+W_{hi}h_{h-1}+W_{ci}c_{t-1}+b_i)\\\\\\\\\n",
        "f_t \u0026 = \\sigma(W_{xf}x_t+W_{hf}h_{h-1}+W_{cf}c_{t-1}+b_f)\\\\\\\\\n",
        "c_t \u0026 = f_t\\odot c_{t-1}+i_t\\odot tanh(W_{xc}x_t+W_{hc}h_{h-1}+b_c)\\\\\\\\\n",
        "o_t \u0026 = \\sigma(W_{xo}x_t+W_{ho}h_{h-1}+W_{co}c_{t}+b_o)\\\\\\\\\n",
        "h_t \u0026 = o_t\\odot tanh(c_t)\\\\\\\\\n",
        "\\end{align}\n",
        "其中，$i_t, f_t, c_t, o_t$分别表示输入门，遗忘门，记忆单元及输出门的向量值，带角标的$W$及$b$为模型参数，$tanh$为双曲正切函数，$\\odot$表示逐元素（elementwise）的乘法操作。输入门控制着新输入进入记忆单元$c$的强度，遗忘门控制着记忆单元维持上一时刻值的强度，输出门控制着输出记忆单元的强度。三种门的计算方式类似，但有着完全不同的参数，它们各自以不同的方式控制着记忆单元$c$，如图2所示：\n",
        "\n",
        "\u003cp align=\"center\"\u003e\n",
        "\u003cimg src=\"image/lstm.png\" width = \"65%\" align=\"center\"/\u003e\u003cbr/\u003e\n",
        "图2. 时刻$t$的LSTM [7]\n",
        "\u003c/p\u003e\n",
        "\n",
        "LSTM通过给简单的循环神经网络增加记忆及控制门的方式，增强了其处理远距离依赖问题的能力。类似原理的改进还有Gated Recurrent Unit (GRU)\\[[8](#参考文献)\\]，其设计更为简洁一些。**这些改进虽然各有不同，但是它们的宏观描述却与简单的循环神经网络一样（如图2所示），即隐状态依据当前输入及前一时刻的隐状态来改变，不断地循环这一过程直至输入处理完毕：**\n",
        "\n",
        "$$ h_t=Recrurent(x_t,h_{t-1})$$\n",
        "\n",
        "其中，$Recrurent$可以表示简单的循环神经网络、GRU或LSTM。\n",
        "\n",
        "### 栈式双向LSTM（Stacked Bidirectional LSTM）\n",
        "\n",
        "对于正常顺序的循环神经网络，$h_t$包含了$t$时刻之前的输入信息，也就是上文信息。同样，为了得到下文信息，我们可以使用反方向（将输入逆序处理）的循环神经网络。结合构建深层循环神经网络的方法（深层神经网络往往能得到更抽象和高级的特征表示），我们可以通过构建更加强有力的基于LSTM的栈式双向循环神经网络\\[[9](#参考文献)\\]，来对时序数据进行建模。  \n",
        "\n",
        "如图3所示（以三层为例），奇数层LSTM正向，偶数层LSTM反向，高一层的LSTM使用低一层LSTM及之前所有层的信息作为输入，对最高层LSTM序列使用时间维度上的最大池化即可得到文本的定长向量表示（这一表示充分融合了文本的上下文信息，并且对文本进行了深层次抽象），最后我们将文本表示连接至softmax构建分类模型。\n",
        "\n",
        "\u003cp align=\"center\"\u003e\n",
        "\u003cimg src=\"image/stacked_lstm.jpg\" width=450\u003e\u003cbr/\u003e\n",
        "图3. 栈式双向LSTM用于文本分类\n",
        "\u003c/p\u003e\n",
        "\n",
        "## 示例程序\n",
        "\n",
        "### 数据集介绍\n",
        "\n",
        "我们以[IMDB情感分析数据集](http://ai.stanford.edu/%7Eamaas/data/sentiment/)为例进行介绍。IMDB数据集的训练集和测试集分别包含25000个已标注过的电影评论。其中，负面评论的得分小于等于4，正面评论的得分大于等于7，满分10分。\n",
        "```text\n",
        "aclImdb\n",
        "|- test\n",
        "   |-- neg\n",
        "   |-- pos\n",
        "|- train\n",
        "   |-- neg\n",
        "   |-- pos\n",
        "```\n",
        "Paddle在`dataset/imdb.py`中提实现了imdb数据集的自动下载和读取，并提供了读取字典、训练数据、测试数据等API。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "import sys\n",
        "import paddle.v2 as paddle\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 配置模型\n",
        "\n",
        "在该示例中，我们实现了两种文本分类算法，分别基于[推荐系统](https://github.com/PaddlePaddle/book/tree/develop/05.recommender_system)一节介绍过的文本卷积神经网络，以及[栈式双向LSTM](#栈式双向LSTM（Stacked Bidirectional LSTM）)。\n",
        "\n",
        "### 文本卷积神经网络\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "def convolution_net(input_dim,\n",
        "                    class_dim=2,\n",
        "                    emb_dim=128,\n",
        "                    hid_dim=128,\n",
        "                    is_predict=False):\n",
        "    data = paddle.layer.data(\"word\",\n",
        "                             paddle.data_type.integer_value_sequence(input_dim))\n",
        "    emb = paddle.layer.embedding(input=data, size=emb_dim)\n",
        "    conv_3 = paddle.networks.sequence_conv_pool(\n",
        "        input=emb, context_len=3, hidden_size=hid_dim)\n",
        "    conv_4 = paddle.networks.sequence_conv_pool(\n",
        "        input=emb, context_len=4, hidden_size=hid_dim)\n",
        "    output = paddle.layer.fc(input=[conv_3, conv_4],\n",
        "                             size=class_dim,\n",
        "                             act=paddle.activation.Softmax())\n",
        "    if not is_predict:\n",
        "        lbl = paddle.layer.data(\"label\", paddle.data_type.integer_value(2))\n",
        "        cost = paddle.layer.classification_cost(input=output, label=lbl)\n",
        "        return cost\n",
        "    else:\n",
        "        return output\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "网络的输入`input_dim`表示的是词典的大小，`class_dim`表示类别数。这里，我们使用[`sequence_conv_pool`](https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/trainer_config_helpers/networks.py) API实现了卷积和池化操作。\n",
        "\n",
        "### 栈式双向LSTM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "def stacked_lstm_net(input_dim,\n",
        "                     class_dim=2,\n",
        "                     emb_dim=128,\n",
        "                     hid_dim=512,\n",
        "                     stacked_num=3,\n",
        "                     is_predict=False):\n",
        "    \"\"\"\n",
        "    A Wrapper for sentiment classification task.\n",
        "    This network uses bi-directional recurrent network,\n",
        "    consisting three LSTM layers. This configure is referred to\n",
        "    the paper as following url, but use fewer layrs.\n",
        "        http://www.aclweb.org/anthology/P15-1109\n",
        "\n",
        "    input_dim: here is word dictionary dimension.\n",
        "    class_dim: number of categories.\n",
        "    emb_dim: dimension of word embedding.\n",
        "    hid_dim: dimension of hidden layer.\n",
        "    stacked_num: number of stacked lstm-hidden layer.\n",
        "    \"\"\"\n",
        "    assert stacked_num % 2 == 1\n",
        "\n",
        "    fc_para_attr = paddle.attr.Param(learning_rate=1e-3)\n",
        "    lstm_para_attr = paddle.attr.Param(initial_std=0., learning_rate=1.)\n",
        "    para_attr = [fc_para_attr, lstm_para_attr]\n",
        "    bias_attr = paddle.attr.Param(initial_std=0., l2_rate=0.)\n",
        "    relu = paddle.activation.Relu()\n",
        "    linear = paddle.activation.Linear()\n",
        "\n",
        "    data = paddle.layer.data(\"word\",\n",
        "                             paddle.data_type.integer_value_sequence(input_dim))\n",
        "    emb = paddle.layer.embedding(input=data, size=emb_dim)\n",
        "\n",
        "    fc1 = paddle.layer.fc(input=emb,\n",
        "                          size=hid_dim,\n",
        "                          act=linear,\n",
        "                          bias_attr=bias_attr)\n",
        "    lstm1 = paddle.layer.lstmemory(\n",
        "        input=fc1, act=relu, bias_attr=bias_attr)\n",
        "\n",
        "    inputs = [fc1, lstm1]\n",
        "    for i in range(2, stacked_num + 1):\n",
        "        fc = paddle.layer.fc(input=inputs,\n",
        "                             size=hid_dim,\n",
        "                             act=linear,\n",
        "                             param_attr=para_attr,\n",
        "                             bias_attr=bias_attr)\n",
        "        lstm = paddle.layer.lstmemory(\n",
        "            input=fc,\n",
        "            reverse=(i % 2) == 0,\n",
        "            act=relu,\n",
        "            bias_attr=bias_attr)\n",
        "        inputs = [fc, lstm]\n",
        "\n",
        "    fc_last = paddle.layer.pooling(input=inputs[0], pooling_type=paddle.pooling.Max())\n",
        "    lstm_last = paddle.layer.pooling(input=inputs[1], pooling_type=paddle.pooling.Max())\n",
        "    output = paddle.layer.fc(input=[fc_last, lstm_last],\n",
        "                             size=class_dim,\n",
        "                             act=paddle.activation.Softmax(),\n",
        "                             bias_attr=bias_attr,\n",
        "                             param_attr=para_attr)\n",
        "\n",
        "    if not is_predict:\n",
        "        lbl = paddle.layer.data(\"label\", paddle.data_type.integer_value(2))\n",
        "        cost = paddle.layer.classification_cost(input=output, label=lbl)\n",
        "        return cost\n",
        "    else:\n",
        "        return output\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "网络的输入`stacked_num`表示的是LSTM的层数，需要是奇数，确保最高层LSTM正向。Paddle里面是通过一个fc和一个lstmemory来实现基于LSTM的循环神经网络。\n",
        "\n",
        "## 训练模型\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # init\n",
        "    paddle.init(use_gpu=False)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "启动paddle程序，use_gpu=False表示用CPU训练，如果系统支持GPU也可以修改成True使用GPU训练。\n",
        "\n",
        "### 训练数据\n",
        "\n",
        "使用Paddle提供的数据集`dataset.imdb`中的API来读取训练数据。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "    print 'load dictionary...'\n",
        "    word_dict = paddle.dataset.imdb.word_dict()\n",
        "    dict_dim = len(word_dict)\n",
        "    class_dim = 2\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "加载数据字典，这里通过`word_dict()`API可以直接构造字典。`class_dim`是指样本类别数，该示例中样本只有正负两类。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "    train_reader = paddle.batch(\n",
        "        paddle.reader.shuffle(\n",
        "            lambda: paddle.dataset.imdb.train(word_dict), buf_size=1000),\n",
        "        batch_size=100)\n",
        "    test_reader = paddle.batch(\n",
        "        lambda: paddle.dataset.imdb.test(word_dict),\n",
        "        batch_size=100)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "这里，`dataset.imdb.train()`和`dataset.imdb.test()`分别是`dataset.imdb`中的训练数据和测试数据API。`train_reader`在训练时使用，意义是将读取的训练数据进行shuffle后，组成一个batch数据。同理，`test_reader`是在测试的时候使用，将读取的测试数据组成一个batch。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "    feeding={'word': 0, 'label': 1}\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`feeding`用来指定`train_reader`和`test_reader`返回的数据与模型配置中data_layer的对应关系。这里表示reader返回的第0列数据对应`word`层，第1列数据对应`label`层。\n",
        "\n",
        "### 构造模型\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "    # Please choose the way to build the network\n",
        "    # by uncommenting the corresponding line.\n",
        "    cost = convolution_net(dict_dim, class_dim=class_dim)\n",
        "    # cost = stacked_lstm_net(dict_dim, class_dim=class_dim, stacked_num=3)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "该示例中默认使用`convolution_net`网络，如果使用`stacked_lstm_net`网络，注释相应的行即可。其中cost是网络的优化目标，同时cost包含了整个网络的拓扑信息。\n",
        "\n",
        "### 网络参数\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "    # create parameters\n",
        "    parameters = paddle.parameters.create(cost)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "根据网络的拓扑构造网络参数。这里parameters是整个网络的参数集。\n",
        "\n",
        "### 优化算法\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "    # create optimizer\n",
        "    adam_optimizer = paddle.optimizer.Adam(\n",
        "        learning_rate=2e-3,\n",
        "        regularization=paddle.optimizer.L2Regularization(rate=8e-4),\n",
        "        model_average=paddle.optimizer.ModelAverage(average_window=0.5))\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Paddle中提供了一系列优化算法的API，这里使用Adam优化算法。\n",
        "\n",
        "### 训练\n",
        "\n",
        "可以通过`paddle.trainer.SGD`构造一个sgd trainer，并调用`trainer.train`来训练模型。另外，通过给train函数传递一个`event_handler`来获取每个batch和每个pass结束的状态。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "    # End batch and end pass event handler\n",
        "    def event_handler(event):\n",
        "        if isinstance(event, paddle.event.EndIteration):\n",
        "            if event.batch_id % 100 == 0:\n",
        "                print \"\\nPass %d, Batch %d, Cost %f, %s\" % (\n",
        "                    event.pass_id, event.batch_id, event.cost, event.metrics)\n",
        "            else:\n",
        "                sys.stdout.write('.')\n",
        "                sys.stdout.flush()\n",
        "        if isinstance(event, paddle.event.EndPass):\n",
        "            with open('./params_pass_%d.tar' % event.pass_id, 'w') as f:\n",
        "                trainer.save_parameter_to_tar(f)\n",
        "\n",
        "            result = trainer.test(reader=test_reader, feeding=feeding)\n",
        "            print \"\\nTest with Pass %d, %s\" % (event.pass_id, result.metrics)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "比如，构造如下一个`event_handler`可以在每100个batch结束后输出cost和error；在每个pass结束后调用`trainer.test`计算一遍测试集并获得当前模型在测试集上的error。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "    from paddle.v2.plot import Ploter\n",
        "\n",
        "    train_title = \"Train cost\"\n",
        "    cost_ploter = Ploter(train_title)\n",
        "    step = 0\n",
        "    def event_handler_plot(event):\n",
        "        global step\n",
        "        if isinstance(event, paddle.event.EndIteration):\n",
        "            cost_ploter.append(train_title, step, event.cost)\n",
        "            cost_ploter.plot()\n",
        "            step += 1\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "或者构造一个`event_handler_plot`画出cost曲线。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "    # create trainer\n",
        "    trainer = paddle.trainer.SGD(cost=cost,\n",
        "                                 parameters=parameters,\n",
        "                                 update_equation=adam_optimizer)\n",
        "\n",
        "    trainer.train(\n",
        "        reader=train_reader,\n",
        "        event_handler=event_handler,\n",
        "        feeding=feeding,\n",
        "        num_passes=2)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "程序运行之后的输出如下。\n",
        "```text\n",
        "Pass 0, Batch 0, Cost 0.693721, {'classification_error_evaluator': 0.5546875}\n",
        "...................................................................................................\n",
        "Pass 0, Batch 100, Cost 0.294321, {'classification_error_evaluator': 0.1015625}\n",
        "...............................................................................................\n",
        "Test with Pass 0, {'classification_error_evaluator': 0.11432000249624252}\n",
        "```\n",
        "\n",
        "## 应用模型\n",
        "\n",
        "可以使用训练好的模型对电影评论进行分类，下面程序展示了如何使用`paddle.infer`接口进行推断。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "    import numpy as np\n",
        "\n",
        "    # Movie Reviews, from imdb test\n",
        "    reviews = [\n",
        "        'Read the book, forget the movie!',\n",
        "        'This is a great movie.'\n",
        "    ]\n",
        "    reviews = [c.split() for c in reviews]\n",
        "\n",
        "    UNK = word_dict['\u003cunk\u003e']\n",
        "    input = []\n",
        "    for c in reviews:\n",
        "        input.append([[word_dict.get(words, UNK) for words in c]])\n",
        "\n",
        "    # 0 stands for positive sample, 1 stands for negative sample\n",
        "    label = {0:'pos', 1:'neg'}\n",
        "    # Use the network used by trainer\n",
        "    out = convolution_net(dict_dim, class_dim=class_dim, is_predict=True)\n",
        "    # out = stacked_lstm_net(dict_dim, class_dim=class_dim, stacked_num=3, is_predict=True)\n",
        "    probs = paddle.infer(output_layer=out, parameters=parameters, input=input)\n",
        "\n",
        "    labs = np.argsort(-probs)\n",
        "    for idx, lab in enumerate(labs):\n",
        "        print idx, \"predicting probability is\", probs[idx], \"label is\", label[lab[0]]\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 总结\n",
        "\n",
        "本章我们以情感分析为例，介绍了使用深度学习的方法进行端对端的短文本分类，并且使用PaddlePaddle完成了全部相关实验。同时，我们简要介绍了两种文本处理模型：卷积神经网络和循环神经网络。在后续的章节中我们会看到这两种基本的深度学习模型在其它任务上的应用。\n",
        "\n",
        "\n",
        "## 参考文献\n",
        "1. Kim Y. [Convolutional neural networks for sentence classification](http://arxiv.org/pdf/1408.5882)[J]. arXiv preprint arXiv:1408.5882, 2014.\n",
        "2. Kalchbrenner N, Grefenstette E, Blunsom P. [A convolutional neural network for modelling sentences](http://arxiv.org/pdf/1404.2188.pdf?utm_medium=App.net\u0026utm_source=PourOver)[J]. arXiv preprint arXiv:1404.2188, 2014.\n",
        "3. Yann N. Dauphin, et al. [Language Modeling with Gated Convolutional Networks](https://arxiv.org/pdf/1612.08083v1.pdf)[J] arXiv preprint arXiv:1612.08083, 2016.\n",
        "4. Siegelmann H T, Sontag E D. [On the computational power of neural nets](http://research.cs.queensu.ca/home/akl/cisc879/papers/SELECTED_PAPERS_FROM_VARIOUS_SOURCES/05070215382317071.pdf)[C]//Proceedings of the fifth annual workshop on Computational learning theory. ACM, 1992: 440-449.\n",
        "5. Hochreiter S, Schmidhuber J. [Long short-term memory](http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf)[J]. Neural computation, 1997, 9(8): 1735-1780.\n",
        "6. Bengio Y, Simard P, Frasconi P. [Learning long-term dependencies with gradient descent is difficult](http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf)[J]. IEEE transactions on neural networks, 1994, 5(2): 157-166.\n",
        "7. Graves A. [Generating sequences with recurrent neural networks](http://arxiv.org/pdf/1308.0850)[J]. arXiv preprint arXiv:1308.0850, 2013.\n",
        "8. Cho K, Van Merriënboer B, Gulcehre C, et al. [Learning phrase representations using RNN encoder-decoder for statistical machine translation](http://arxiv.org/pdf/1406.1078)[J]. arXiv preprint arXiv:1406.1078, 2014.\n",
        "9. Zhou J, Xu W. [End-to-end learning of semantic role labeling using recurrent neural networks](http://www.aclweb.org/anthology/P/P15/P15-1109.pdf)[C]//Proceedings of the Annual Meeting of the Association for Computational Linguistics. 2015.\n",
        "\n",
        "\u003cbr/\u003e\n",
        "\u003ca rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"\u003e\u003cimg alt=\"知识共享许可协议\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /\u003e\u003c/a\u003e\u003cbr /\u003e\u003cspan xmlns:dct=\"http://purl.org/dc/terms/\" href=\"http://purl.org/dc/dcmitype/Text\" property=\"dct:title\" rel=\"dct:type\"\u003e本教程\u003c/span\u003e 由 \u003ca xmlns:cc=\"http://creativecommons.org/ns#\" href=\"http://book.paddlepaddle.org\" property=\"cc:attributionName\" rel=\"cc:attributionURL\"\u003ePaddlePaddle\u003c/a\u003e 创作，采用 \u003ca rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"\u003e知识共享 署名-相同方式共享 4.0 国际 许可协议\u003c/a\u003e进行许可。\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
